{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OSp18hvBgSg"
   },
   "source": [
    "* Read Language model tutorial ---> https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
    "* Find one english corpus with poetries in the internet (e.g from here) --> https://www.poetryfoundation.org/poems\n",
    "* You can use whatever corpus you want (e.g. your favorite book)\n",
    "* Encapsulate LSTM building like MLP from the first task\n",
    "* Train LSTM as language model on your corpus like in the tutorial\n",
    "* Also, you need to compare 1-layer and 2-layer LSTMs\n",
    "* Compare texts, generated by your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:05.096512Z",
     "iopub.status.busy": "2022-11-20T05:16:05.096270Z",
     "iopub.status.idle": "2022-11-20T05:16:06.732699Z",
     "shell.execute_reply": "2022-11-20T05:16:06.731685Z",
     "shell.execute_reply.started": "2022-11-20T05:16:05.096464Z"
    },
    "id": "G3rdTupmBgSj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import adam, adagrad, adadelta, rmsprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:23.751772Z",
     "iopub.status.busy": "2022-11-20T05:16:23.751449Z",
     "iopub.status.idle": "2022-11-20T05:16:23.756989Z",
     "shell.execute_reply": "2022-11-20T05:16:23.755693Z",
     "shell.execute_reply.started": "2022-11-20T05:16:23.751716Z"
    },
    "id": "_KXJ2f9jBgSu"
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "folder_name = '../input'\n",
    "filename = os.path.join(folder_name, 'task3_corpus.csv')\n",
    "# filename = 'task3_corpus.csv'\n",
    "file_type = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:29.067695Z",
     "iopub.status.busy": "2022-11-20T05:16:29.067401Z",
     "iopub.status.idle": "2022-11-20T05:16:29.073046Z",
     "shell.execute_reply": "2022-11-20T05:16:29.071062Z",
     "shell.execute_reply.started": "2022-11-20T05:16:29.067630Z"
    },
    "id": "_fW7IwapBgS0"
   },
   "outputs": [],
   "source": [
    "def read_data(filename, file_type):\n",
    "    if file_type == 'csv':\n",
    "        data = pd.read_csv(filename)\n",
    "        data = data['text']\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:29.742401Z",
     "iopub.status.busy": "2022-11-20T05:16:29.742128Z",
     "iopub.status.idle": "2022-11-20T05:16:29.765932Z",
     "shell.execute_reply": "2022-11-20T05:16:29.765160Z",
     "shell.execute_reply.started": "2022-11-20T05:16:29.742351Z"
    },
    "id": "hwGWpRi_BgTM"
   },
   "outputs": [],
   "source": [
    "df = read_data(filename, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:30.588346Z",
     "iopub.status.busy": "2022-11-20T05:16:30.588063Z",
     "iopub.status.idle": "2022-11-20T05:16:30.592429Z",
     "shell.execute_reply": "2022-11-20T05:16:30.591496Z",
     "shell.execute_reply.started": "2022-11-20T05:16:30.588296Z"
    },
    "id": "qDLP6YnyBgUE"
   },
   "outputs": [],
   "source": [
    "text = '\\n'.join([row for row in df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:31.542293Z",
     "iopub.status.busy": "2022-11-20T05:16:31.542011Z",
     "iopub.status.idle": "2022-11-20T05:16:31.550252Z",
     "shell.execute_reply": "2022-11-20T05:16:31.549383Z",
     "shell.execute_reply.started": "2022-11-20T05:16:31.542236Z"
    },
    "id": "REwgMEzJBgUU",
    "outputId": "9540e057-9234-4efe-9034-ae937acfcad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "layers = [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)]\n",
    "count = [x for x,_ in layers].count('LSTM')\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:32.772968Z",
     "iopub.status.busy": "2022-11-20T05:16:32.772647Z",
     "iopub.status.idle": "2022-11-20T05:16:32.785815Z",
     "shell.execute_reply": "2022-11-20T05:16:32.784599Z",
     "shell.execute_reply.started": "2022-11-20T05:16:32.772914Z"
    },
    "id": "17Z8RQoVBgUl"
   },
   "outputs": [],
   "source": [
    "class ModelFormer:\n",
    "    def __init__(self):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.best_model = Sequential()\n",
    "        self.best_accuracy = 0\n",
    "        self.best_parameters = {}\n",
    "        \n",
    "    def fit_data(self, text):\n",
    "        self.original_corpus = text\n",
    "        self.corpus = self.original_corpus.lower().split('\\n')\n",
    "        self.tokenizer.fit_on_texts(self.corpus)\n",
    "        self.word_count = len(self.tokenizer.word_index) + 1\n",
    "        input_sequences = []\n",
    "        for line in self.corpus:\n",
    "            tokens = self.tokenizer.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(tokens)):\n",
    "                n_grams_sequence = tokens[:i+1]\n",
    "                input_sequences.append(n_grams_sequence)\n",
    "        \n",
    "        input_sequences = self.pad_input_sequences(input_sequences)\n",
    "        \n",
    "        x_data, y_data = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "        y_data = np_utils.to_categorical(y_data, num_classes=self.word_count)\n",
    "        \n",
    "        return x_data, y_data\n",
    "              \n",
    "    def pad_input_sequences(self,input_sequences):\n",
    "        max_sequence_length = max([len(sentence) for sentence in input_sequences])\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
    "        return input_sequences\n",
    "    \n",
    "    def fit(self, x_data, y_data , layers= [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)], activation='tanh', optimizer='adam', lr=0.01, epochs=20):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        x_train, x_val, y_train, y_val = train_test_split(self.x_data, self.y_data)\n",
    "        \n",
    "        \n",
    "        self.model.add(Embedding(self.word_count, 10, input_length=len(x_data[0]) ))\n",
    "        count_lstm_retn_flag = [x for x,_ in layers].count('LSTM') - 1\n",
    "\n",
    "        for layer,value in layers:\n",
    "            if layer == 'LSTM':\n",
    "                if count_lstm_retn_flag:\n",
    "                    count_lstm_retn_flag -= 1\n",
    "                    return_sequences = True \n",
    "                else:\n",
    "                    return_sequences = False\n",
    "                self.model.add(LSTM(value, activation=activation, return_sequences=return_sequences))\n",
    "            if layer == 'Dropout':\n",
    "                self.model.add(Dropout(value))\n",
    "        \n",
    "        self.model.add(Dense(self.word_count, activation='softmax'))\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = adam(lr=lr)\n",
    "        elif optimizer == 'adadelta':\n",
    "            optimizer = adadelta(lr=lr)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            optimizer = rmsprop(lr=lr)\n",
    "            \n",
    "            \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "        fit_summary = self.model.fit(x_train, y_train, epochs=epochs, verbose=0, validation_data=(x_val, y_val), batch_size=50)\n",
    "        if fit_summary.history['acc'][-1] > self.best_accuracy:\n",
    "            self.best_model = self.model\n",
    "            self.best_accuracy = fit_summary.history['acc'][-1]\n",
    "            self.best_parameters = (layers, activation, optimizer, lr, epochs)\n",
    "        \n",
    "        return fit_summary\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:33.457712Z",
     "iopub.status.busy": "2022-11-20T05:16:33.457419Z",
     "iopub.status.idle": "2022-11-20T05:16:33.696933Z",
     "shell.execute_reply": "2022-11-20T05:16:33.696037Z",
     "shell.execute_reply.started": "2022-11-20T05:16:33.457645Z"
    },
    "id": "um7C_bMBBgUs"
   },
   "outputs": [],
   "source": [
    "m = ModelFormer()\n",
    "X, Y= m.fit_data(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:34.174322Z",
     "iopub.status.busy": "2022-11-20T05:16:34.174057Z",
     "iopub.status.idle": "2022-11-20T05:16:34.275876Z",
     "shell.execute_reply": "2022-11-20T05:16:34.275150Z",
     "shell.execute_reply.started": "2022-11-20T05:16:34.174273Z"
    },
    "id": "IxxUqsczBgU3"
   },
   "outputs": [],
   "source": [
    "x_train , x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:35.719776Z",
     "iopub.status.busy": "2022-11-20T05:16:35.719358Z",
     "iopub.status.idle": "2022-11-20T05:16:35.728629Z",
     "shell.execute_reply": "2022-11-20T05:16:35.727471Z",
     "shell.execute_reply.started": "2022-11-20T05:16:35.719689Z"
    },
    "id": "bEvPZQedBgVA",
    "outputId": "e1f1e726-08a0-4b7d-fcc8-432955c1353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8847 8847 3792 3792\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train), len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:36.872420Z",
     "iopub.status.busy": "2022-11-20T05:16:36.872136Z",
     "iopub.status.idle": "2022-11-20T05:16:36.876191Z",
     "shell.execute_reply": "2022-11-20T05:16:36.875142Z",
     "shell.execute_reply.started": "2022-11-20T05:16:36.872368Z"
    },
    "id": "vMKPolLxBgWM"
   },
   "outputs": [],
   "source": [
    "# Define Grid Search with Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:37.542833Z",
     "iopub.status.busy": "2022-11-20T05:16:37.542523Z",
     "iopub.status.idle": "2022-11-20T05:16:37.549661Z",
     "shell.execute_reply": "2022-11-20T05:16:37.548736Z",
     "shell.execute_reply.started": "2022-11-20T05:16:37.542778Z"
    },
    "id": "yOAkMxdABgWa"
   },
   "outputs": [],
   "source": [
    "hyperparameters = { 'layers': [ [( 'LSTM', 200), ('Dropout', 0.2)], [( 'LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2) ]], \n",
    "                     'activation': ['tanh'],\n",
    "                     'optimizer' : [ ('adam', 0.01 ), ('adam', 0.001 ) , ('adadelta', 1 ), ('rmsprop', 0.1 )],\n",
    "                     'epochs' : [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:38.928526Z",
     "iopub.status.busy": "2022-11-20T05:16:38.928161Z",
     "iopub.status.idle": "2022-11-20T05:16:38.937927Z",
     "shell.execute_reply": "2022-11-20T05:16:38.937098Z",
     "shell.execute_reply.started": "2022-11-20T05:16:38.928449Z"
    },
    "id": "unOtt-QJBgWn",
    "outputId": "ef06ce53-9d57-4a3b-98bc-efa4dab3c5e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adam', 0.01)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adam', 0.001)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adadelta', 1)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
       "  'optimizer': ('rmsprop', 0.1)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adam', 0.01)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adam', 0.001)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
       "  'optimizer': ('adadelta', 1)},\n",
       " {'activation': 'tanh',\n",
       "  'epochs': 50,\n",
       "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
       "  'optimizer': ('rmsprop', 0.1)}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations = list(ParameterGrid(hyperparameters))\n",
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:39.752363Z",
     "iopub.status.busy": "2022-11-20T05:16:39.752084Z",
     "iopub.status.idle": "2022-11-20T05:16:39.756098Z",
     "shell.execute_reply": "2022-11-20T05:16:39.755339Z",
     "shell.execute_reply.started": "2022-11-20T05:16:39.752311Z"
    },
    "id": "EmkHhKE0WiQY"
   },
   "outputs": [],
   "source": [
    "fit_summary_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4760
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T05:16:40.623079Z",
     "iopub.status.busy": "2022-11-20T05:16:40.622787Z",
     "iopub.status.idle": "2022-11-20T06:54:50.281164Z",
     "shell.execute_reply": "2022-11-20T06:54:50.280308Z",
     "shell.execute_reply.started": "2022-11-20T05:16:40.623029Z"
    },
    "id": "EApzDLTlXP5s",
    "outputId": "b8a78396-6fb5-407e-91ea-700bbf976a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.01)}\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2185)              439185    \n",
      "=================================================================\n",
      "Total params: 629,835\n",
      "Trainable params: 629,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.001)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2185)              439185    \n",
      "=================================================================\n",
      "Total params: 629,835\n",
      "Trainable params: 629,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adadelta', 1)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2185)              439185    \n",
      "=================================================================\n",
      "Total params: 629,835\n",
      "Trainable params: 629,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('rmsprop', 0.1)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2185)              439185    \n",
      "=================================================================\n",
      "Total params: 629,835\n",
      "Trainable params: 629,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adam', 0.01)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 54, 200)           168800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 54, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2185)              876185    \n",
      "=================================================================\n",
      "Total params: 2,028,435\n",
      "Trainable params: 2,028,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adam', 0.001)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 54, 200)           168800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 54, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2185)              876185    \n",
      "=================================================================\n",
      "Total params: 2,028,435\n",
      "Trainable params: 2,028,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adadelta', 1)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 54, 200)           168800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 54, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2185)              876185    \n",
      "=================================================================\n",
      "Total params: 2,028,435\n",
      "Trainable params: 2,028,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('rmsprop', 0.1)}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 54, 10)            21850     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 54, 200)           168800    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 54, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2185)              876185    \n",
      "=================================================================\n",
      "Total params: 2,028,435\n",
      "Trainable params: 2,028,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for combination in combinations:\n",
    "    print('Current Combination : {}'.format(combination))\n",
    "    fit_summary_array.append(m.fit(x_train, y_train, layers=combination['layers'], activation=combination['activation'], optimizer=combination['optimizer'][0], lr=combination['optimizer'][1], epochs=combination['epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T07:16:56.276599Z",
     "iopub.status.busy": "2022-11-20T07:16:56.276300Z",
     "iopub.status.idle": "2022-11-20T07:16:56.281188Z",
     "shell.execute_reply": "2022-11-20T07:16:56.280170Z",
     "shell.execute_reply.started": "2022-11-20T07:16:56.276549Z"
    },
    "id": "la73qbwFICva",
    "outputId": "f7f9e723-c6a6-403b-8ee9-01fc182042c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy : 70.20346670107637, with best Parameters : ([('LSTM', 200), ('Dropout', 0.2)], 'tanh', <keras.optimizers.Adam object at 0x7f8ee00bbe10>, 0.001, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Best Accuracy : {}, with best Parameters : {}'.format(m.best_accuracy*100, m.best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T07:16:56.958592Z",
     "iopub.status.busy": "2022-11-20T07:16:56.958310Z",
     "iopub.status.idle": "2022-11-20T07:16:56.965119Z",
     "shell.execute_reply": "2022-11-20T07:16:56.963973Z",
     "shell.execute_reply.started": "2022-11-20T07:16:56.958537Z"
    },
    "id": "A69KBMuuYCKc"
   },
   "outputs": [],
   "source": [
    "# Generate Sentences : \n",
    "def generate_n_sentences(n=5):\n",
    "    final_sentences = []\n",
    "    for _ in range(n):\n",
    "        prediction = x_test[np.random.randint(len(x_test))]\n",
    "        prediction = np.delete(prediction, 0)\n",
    "        first_prediction = m.best_model.predict_classes([x_test[0].reshape(1,54)])\n",
    "        prediction = np.append(prediction,first_prediction)\n",
    "        for _ in range(5):\n",
    "            next_prediction = m.best_model.predict_classes(prediction.reshape(1,54))\n",
    "            prediction = np.delete(prediction, 0)\n",
    "            prediction = np.append(prediction,next_prediction)\n",
    "\n",
    "\n",
    "\n",
    "        output_word = \"\"\n",
    "        for i in prediction:\n",
    "            if i:\n",
    "                for word,index in m.tokenizer.word_index.items():\n",
    "                    if index == i:\n",
    "                        output_word += word + ' '\n",
    "                        break\n",
    "\n",
    "        final_sentences.append(output_word)\n",
    "    return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-11-20T07:16:58.459700Z",
     "iopub.status.busy": "2022-11-20T07:16:58.459407Z",
     "iopub.status.idle": "2022-11-20T07:17:00.547847Z",
     "shell.execute_reply": "2022-11-20T07:17:00.547068Z",
     "shell.execute_reply.started": "2022-11-20T07:16:58.459636Z"
    },
    "id": "R6dDdSXlrxKc",
    "outputId": "125e9b97-781d-4baf-81ab-6cd466e0b505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['presidents have been trying unsuccessfully for years to get germany and other rich nato nations to pay more toward choice or work for the donald ',\n",
       " 'finally liddle’ adam schiff the leakin’ monster of no control is now blaming the obama administration for russian meddling in the 2016 election he is finally right about something obama was president knew of the threat and did nothing thank choice to spy and a reasons ',\n",
       " 'the mainstream media is under fire and being scorned all over the world as being corrupt and fake for two years they pushed the russian collusion choice they existed be pushing a ',\n",
       " 'now that they realize the only collusion with russia was done by crooked hillary clinton amp the democrats nadler schiff and the dem heads of the committees have gone stone cold crazy 81 letter sent choice to are it’s are acceptable ',\n",
       " 'the fake news choice to talk to multiple media ',\n",
       " 'the choice with russia and the whole ',\n",
       " 'our relationship with russia is worse now than it has ever been and that includes the cold war choice why did there was no ',\n",
       " 'president macron of france has just suggested that europe build its own military choice their do an emails on ',\n",
       " 'question if all of the russian meddling took place during the obama administration right up to january 20th why aren’t they the subject choice the election refused why next ',\n",
       " 'what about all of the clinton ties to choice with 18 long of russia ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n_sentences(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQ2AjP6XIQMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Task 3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
