{"metadata":{"accelerator":"GPU","colab":{"name":"Task 3.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Read Language model tutorial ---> https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n* Find one english corpus with poetries in the internet (e.g from here) --> https://www.poetryfoundation.org/poems\n* You can use whatever corpus you want (e.g. your favorite book)\n* Encapsulate LSTM building like MLP from the first task\n* Train LSTM as language model on your corpus like in the tutorial\n* Also, you need to compare 1-layer and 2-layer LSTMs\n* Compare texts, generated by your models","metadata":{"colab_type":"text","id":"5OSp18hvBgSg"}},{"cell_type":"code","source":"from __future__ import print_function\n\nimport keras\nfrom keras.layers import Input, Embedding, LSTM, Dense, Dropout\nfrom keras.utils import np_utils\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.optimizers import adam, adagrad, adadelta, rmsprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.regularizers import L1L2\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import ParameterGrid, train_test_split\n\nimport os\nimport numpy as np\nimport pandas as pd\n\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","metadata":{"colab":{},"colab_type":"code","id":"G3rdTupmBgSj","execution":{"iopub.status.busy":"2022-11-20T05:16:05.096270Z","iopub.execute_input":"2022-11-20T05:16:05.096512Z","iopub.status.idle":"2022-11-20T05:16:06.732699Z","shell.execute_reply.started":"2022-11-20T05:16:05.096464Z","shell.execute_reply":"2022-11-20T05:16:06.731685Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read Data\nfolder_name = '../input'\nfilename = os.path.join(folder_name, 'task3_corpus.csv')\n# filename = 'task3_corpus.csv'\nfile_type = 'csv'","metadata":{"colab":{},"colab_type":"code","id":"_KXJ2f9jBgSu","execution":{"iopub.status.busy":"2022-11-20T05:16:23.751449Z","iopub.execute_input":"2022-11-20T05:16:23.751772Z","iopub.status.idle":"2022-11-20T05:16:23.756989Z","shell.execute_reply.started":"2022-11-20T05:16:23.751716Z","shell.execute_reply":"2022-11-20T05:16:23.755693Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def read_data(filename, file_type):\n    if file_type == 'csv':\n        data = pd.read_csv(filename)\n        data = data['text']\n        \n    return data","metadata":{"colab":{},"colab_type":"code","id":"_fW7IwapBgS0","execution":{"iopub.status.busy":"2022-11-20T05:16:29.067401Z","iopub.execute_input":"2022-11-20T05:16:29.067695Z","iopub.status.idle":"2022-11-20T05:16:29.073046Z","shell.execute_reply.started":"2022-11-20T05:16:29.067630Z","shell.execute_reply":"2022-11-20T05:16:29.071062Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = read_data(filename, file_type)","metadata":{"colab":{},"colab_type":"code","id":"hwGWpRi_BgTM","execution":{"iopub.status.busy":"2022-11-20T05:16:29.742128Z","iopub.execute_input":"2022-11-20T05:16:29.742401Z","iopub.status.idle":"2022-11-20T05:16:29.765932Z","shell.execute_reply.started":"2022-11-20T05:16:29.742351Z","shell.execute_reply":"2022-11-20T05:16:29.765160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"text = '\\n'.join([row for row in df])","metadata":{"colab":{},"colab_type":"code","id":"qDLP6YnyBgUE","execution":{"iopub.status.busy":"2022-11-20T05:16:30.588063Z","iopub.execute_input":"2022-11-20T05:16:30.588346Z","iopub.status.idle":"2022-11-20T05:16:30.592429Z","shell.execute_reply.started":"2022-11-20T05:16:30.588296Z","shell.execute_reply":"2022-11-20T05:16:30.591496Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"layers = [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)]\ncount = [x for x,_ in layers].count('LSTM')\nprint(count)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"REwgMEzJBgUU","outputId":"9540e057-9234-4efe-9034-ae937acfcad0","execution":{"iopub.status.busy":"2022-11-20T05:16:31.542011Z","iopub.execute_input":"2022-11-20T05:16:31.542293Z","iopub.status.idle":"2022-11-20T05:16:31.550252Z","shell.execute_reply.started":"2022-11-20T05:16:31.542236Z","shell.execute_reply":"2022-11-20T05:16:31.549383Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"class ModelFormer:\n    def __init__(self):\n        self.x = []\n        self.y = []\n        self.tokenizer = Tokenizer()\n        self.best_model = Sequential()\n        self.best_accuracy = 0\n        self.best_parameters = {}\n        \n    def fit_data(self, text):\n        self.original_corpus = text\n        self.corpus = self.original_corpus.lower().split('\\n')\n        self.tokenizer.fit_on_texts(self.corpus)\n        self.word_count = len(self.tokenizer.word_index) + 1\n        input_sequences = []\n        for line in self.corpus:\n            tokens = self.tokenizer.texts_to_sequences([line])[0]\n            for i in range(1, len(tokens)):\n                n_grams_sequence = tokens[:i+1]\n                input_sequences.append(n_grams_sequence)\n        \n        input_sequences = self.pad_input_sequences(input_sequences)\n        \n        x_data, y_data = input_sequences[:,:-1], input_sequences[:,-1]\n        y_data = np_utils.to_categorical(y_data, num_classes=self.word_count)\n        \n        return x_data, y_data\n              \n    def pad_input_sequences(self,input_sequences):\n        max_sequence_length = max([len(sentence) for sentence in input_sequences])\n        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n        return input_sequences\n    \n    def fit(self, x_data, y_data , layers= [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)], activation='tanh', optimizer='adam', lr=0.01, epochs=20):\n        self.model = Sequential()\n        \n        self.x_data = x_data\n        self.y_data = y_data\n        x_train, x_val, y_train, y_val = train_test_split(self.x_data, self.y_data)\n        \n        \n        self.model.add(Embedding(self.word_count, 10, input_length=len(x_data[0]) ))\n        count_lstm_retn_flag = [x for x,_ in layers].count('LSTM') - 1\n\n        for layer,value in layers:\n            if layer == 'LSTM':\n                if count_lstm_retn_flag:\n                    count_lstm_retn_flag -= 1\n                    return_sequences = True \n                else:\n                    return_sequences = False\n                self.model.add(LSTM(value, activation=activation, return_sequences=return_sequences))\n            if layer == 'Dropout':\n                self.model.add(Dropout(value))\n        \n        self.model.add(Dense(self.word_count, activation='softmax'))\n        if optimizer == 'adam':\n            optimizer = adam(lr=lr)\n        elif optimizer == 'adadelta':\n            optimizer = adadelta(lr=lr)\n        elif optimizer == 'rmsprop':\n            optimizer = rmsprop(lr=lr)\n            \n            \n        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        self.model.summary()\n        \n        fit_summary = self.model.fit(x_train, y_train, epochs=epochs, verbose=0, validation_data=(x_val, y_val), batch_size=50)\n        if fit_summary.history['acc'][-1] > self.best_accuracy:\n            self.best_model = self.model\n            self.best_accuracy = fit_summary.history['acc'][-1]\n            self.best_parameters = (layers, activation, optimizer, lr, epochs)\n        \n        return fit_summary\n        \n        ","metadata":{"colab":{},"colab_type":"code","id":"17Z8RQoVBgUl","execution":{"iopub.status.busy":"2022-11-20T05:16:32.772647Z","iopub.execute_input":"2022-11-20T05:16:32.772968Z","iopub.status.idle":"2022-11-20T05:16:32.785815Z","shell.execute_reply.started":"2022-11-20T05:16:32.772914Z","shell.execute_reply":"2022-11-20T05:16:32.784599Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"m = ModelFormer()\nX, Y= m.fit_data(text)\n","metadata":{"colab":{},"colab_type":"code","id":"um7C_bMBBgUs","execution":{"iopub.status.busy":"2022-11-20T05:16:33.457419Z","iopub.execute_input":"2022-11-20T05:16:33.457712Z","iopub.status.idle":"2022-11-20T05:16:33.696933Z","shell.execute_reply.started":"2022-11-20T05:16:33.457645Z","shell.execute_reply":"2022-11-20T05:16:33.696037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"x_train , x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)","metadata":{"colab":{},"colab_type":"code","id":"IxxUqsczBgU3","execution":{"iopub.status.busy":"2022-11-20T05:16:34.174057Z","iopub.execute_input":"2022-11-20T05:16:34.174322Z","iopub.status.idle":"2022-11-20T05:16:34.275876Z","shell.execute_reply.started":"2022-11-20T05:16:34.174273Z","shell.execute_reply":"2022-11-20T05:16:34.275150Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(len(x_train), len(y_train), len(x_test), len(y_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"bEvPZQedBgVA","outputId":"e1f1e726-08a0-4b7d-fcc8-432955c1353e","execution":{"iopub.status.busy":"2022-11-20T05:16:35.719358Z","iopub.execute_input":"2022-11-20T05:16:35.719776Z","iopub.status.idle":"2022-11-20T05:16:35.728629Z","shell.execute_reply.started":"2022-11-20T05:16:35.719689Z","shell.execute_reply":"2022-11-20T05:16:35.727471Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"8847 8847 3792 3792\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define Grid Search with Parameter Grid","metadata":{"colab":{},"colab_type":"code","id":"vMKPolLxBgWM","execution":{"iopub.status.busy":"2022-11-20T05:16:36.872136Z","iopub.execute_input":"2022-11-20T05:16:36.872420Z","iopub.status.idle":"2022-11-20T05:16:36.876191Z","shell.execute_reply.started":"2022-11-20T05:16:36.872368Z","shell.execute_reply":"2022-11-20T05:16:36.875142Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"hyperparameters = { 'layers': [ [( 'LSTM', 200), ('Dropout', 0.2)], [( 'LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2) ]], \n                     'activation': ['tanh'],\n                     'optimizer' : [ ('adam', 0.01 ), ('adam', 0.001 ) , ('adadelta', 1 ), ('rmsprop', 0.1 )],\n                     'epochs' : [50]\n                   }","metadata":{"colab":{},"colab_type":"code","id":"yOAkMxdABgWa","execution":{"iopub.status.busy":"2022-11-20T05:16:37.542523Z","iopub.execute_input":"2022-11-20T05:16:37.542833Z","iopub.status.idle":"2022-11-20T05:16:37.549661Z","shell.execute_reply.started":"2022-11-20T05:16:37.542778Z","shell.execute_reply":"2022-11-20T05:16:37.548736Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"combinations = list(ParameterGrid(hyperparameters))\ncombinations","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"colab_type":"code","id":"unOtt-QJBgWn","outputId":"ef06ce53-9d57-4a3b-98bc-efa4dab3c5e2","execution":{"iopub.status.busy":"2022-11-20T05:16:38.928161Z","iopub.execute_input":"2022-11-20T05:16:38.928526Z","iopub.status.idle":"2022-11-20T05:16:38.937927Z","shell.execute_reply.started":"2022-11-20T05:16:38.928449Z","shell.execute_reply":"2022-11-20T05:16:38.937098Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n  'optimizer': ('adam', 0.01)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n  'optimizer': ('adam', 0.001)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n  'optimizer': ('adadelta', 1)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n  'optimizer': ('rmsprop', 0.1)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n  'optimizer': ('adam', 0.01)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n  'optimizer': ('adam', 0.001)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n  'optimizer': ('adadelta', 1)},\n {'activation': 'tanh',\n  'epochs': 50,\n  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n  'optimizer': ('rmsprop', 0.1)}]"},"metadata":{}}]},{"cell_type":"code","source":"fit_summary_array = []","metadata":{"colab":{},"colab_type":"code","id":"EmkHhKE0WiQY","execution":{"iopub.status.busy":"2022-11-20T05:16:39.752084Z","iopub.execute_input":"2022-11-20T05:16:39.752363Z","iopub.status.idle":"2022-11-20T05:16:39.756098Z","shell.execute_reply.started":"2022-11-20T05:16:39.752311Z","shell.execute_reply":"2022-11-20T05:16:39.755339Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for combination in combinations:\n    print('Current Combination : {}'.format(combination))\n    fit_summary_array.append(m.fit(x_train, y_train, layers=combination['layers'], activation=combination['activation'], optimizer=combination['optimizer'][0], lr=combination['optimizer'][1], epochs=combination['epochs']))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":4760},"colab_type":"code","id":"EApzDLTlXP5s","outputId":"b8a78396-6fb5-407e-91ea-700bbf976a21","execution":{"iopub.status.busy":"2022-11-20T05:16:40.622787Z","iopub.execute_input":"2022-11-20T05:16:40.623079Z","iopub.status.idle":"2022-11-20T06:54:50.281164Z","shell.execute_reply.started":"2022-11-20T05:16:40.623029Z","shell.execute_reply":"2022-11-20T06:54:50.280308Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.01)}\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 200)               168800    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 200)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2185)              439185    \n=================================================================\nTotal params: 629,835\nTrainable params: 629,835\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.001)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 200)               168800    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 200)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2185)              439185    \n=================================================================\nTotal params: 629,835\nTrainable params: 629,835\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adadelta', 1)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 200)               168800    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 200)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2185)              439185    \n=================================================================\nTotal params: 629,835\nTrainable params: 629,835\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('rmsprop', 0.1)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 200)               168800    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 200)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 2185)              439185    \n=================================================================\nTotal params: 629,835\nTrainable params: 629,835\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adam', 0.01)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 54, 200)           168800    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 54, 200)           0         \n_________________________________________________________________\nlstm_6 (LSTM)                (None, 400)               961600    \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 400)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 2185)              876185    \n=================================================================\nTotal params: 2,028,435\nTrainable params: 2,028,435\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adam', 0.001)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 54, 200)           168800    \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 54, 200)           0         \n_________________________________________________________________\nlstm_8 (LSTM)                (None, 400)               961600    \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 400)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 2185)              876185    \n=================================================================\nTotal params: 2,028,435\nTrainable params: 2,028,435\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('adadelta', 1)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_9 (LSTM)                (None, 54, 200)           168800    \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 54, 200)           0         \n_________________________________________________________________\nlstm_10 (LSTM)               (None, 400)               961600    \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 400)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 2185)              876185    \n=================================================================\nTotal params: 2,028,435\nTrainable params: 2,028,435\nNon-trainable params: 0\n_________________________________________________________________\nCurrent Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)], 'optimizer': ('rmsprop', 0.1)}\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_8 (Embedding)      (None, 54, 10)            21850     \n_________________________________________________________________\nlstm_11 (LSTM)               (None, 54, 200)           168800    \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 54, 200)           0         \n_________________________________________________________________\nlstm_12 (LSTM)               (None, 400)               961600    \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 400)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 2185)              876185    \n=================================================================\nTotal params: 2,028,435\nTrainable params: 2,028,435\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Best Accuracy : {}, with best Parameters : {}'.format(m.best_accuracy*100, m.best_parameters))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"la73qbwFICva","outputId":"f7f9e723-c6a6-403b-8ee9-01fc182042c7","execution":{"iopub.status.busy":"2022-11-20T07:16:56.276300Z","iopub.execute_input":"2022-11-20T07:16:56.276599Z","iopub.status.idle":"2022-11-20T07:16:56.281188Z","shell.execute_reply.started":"2022-11-20T07:16:56.276549Z","shell.execute_reply":"2022-11-20T07:16:56.280170Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Best Accuracy : 70.20346670107637, with best Parameters : ([('LSTM', 200), ('Dropout', 0.2)], 'tanh', <keras.optimizers.Adam object at 0x7f8ee00bbe10>, 0.001, 50)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate Sentences : \ndef generate_n_sentences(n=5):\n    final_sentences = []\n    for _ in range(n):\n        prediction = x_test[np.random.randint(len(x_test))]\n        prediction = np.delete(prediction, 0)\n        first_prediction = m.best_model.predict_classes([x_test[0].reshape(1,54)])\n        prediction = np.append(prediction,first_prediction)\n        for _ in range(5):\n            next_prediction = m.best_model.predict_classes(prediction.reshape(1,54))\n            prediction = np.delete(prediction, 0)\n            prediction = np.append(prediction,next_prediction)\n\n\n\n        output_word = \"\"\n        for i in prediction:\n            if i:\n                for word,index in m.tokenizer.word_index.items():\n                    if index == i:\n                        output_word += word + ' '\n                        break\n\n        final_sentences.append(output_word)\n    return final_sentences","metadata":{"colab":{},"colab_type":"code","id":"A69KBMuuYCKc","execution":{"iopub.status.busy":"2022-11-20T07:16:56.958310Z","iopub.execute_input":"2022-11-20T07:16:56.958592Z","iopub.status.idle":"2022-11-20T07:16:56.965119Z","shell.execute_reply.started":"2022-11-20T07:16:56.958537Z","shell.execute_reply":"2022-11-20T07:16:56.963973Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"generate_n_sentences(10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"colab_type":"code","id":"R6dDdSXlrxKc","outputId":"125e9b97-781d-4baf-81ab-6cd466e0b505","execution":{"iopub.status.busy":"2022-11-20T07:16:58.459407Z","iopub.execute_input":"2022-11-20T07:16:58.459700Z","iopub.status.idle":"2022-11-20T07:17:00.547847Z","shell.execute_reply.started":"2022-11-20T07:16:58.459636Z","shell.execute_reply":"2022-11-20T07:17:00.547068Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['presidents have been trying unsuccessfully for years to get germany and other rich nato nations to pay more toward choice or work for the donald ',\n 'finally liddle’ adam schiff the leakin’ monster of no control is now blaming the obama administration for russian meddling in the 2016 election he is finally right about something obama was president knew of the threat and did nothing thank choice to spy and a reasons ',\n 'the mainstream media is under fire and being scorned all over the world as being corrupt and fake for two years they pushed the russian collusion choice they existed be pushing a ',\n 'now that they realize the only collusion with russia was done by crooked hillary clinton amp the democrats nadler schiff and the dem heads of the committees have gone stone cold crazy 81 letter sent choice to are it’s are acceptable ',\n 'the fake news choice to talk to multiple media ',\n 'the choice with russia and the whole ',\n 'our relationship with russia is worse now than it has ever been and that includes the cold war choice why did there was no ',\n 'president macron of france has just suggested that europe build its own military choice their do an emails on ',\n 'question if all of the russian meddling took place during the obama administration right up to january 20th why aren’t they the subject choice the election refused why next ',\n 'what about all of the clinton ties to choice with 18 long of russia ']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"colab":{},"colab_type":"code","id":"KQ2AjP6XIQMQ","trusted":true},"execution_count":null,"outputs":[]}]}